{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Autoencoder import *\n",
    "from dataprocess import *\n",
    "from train import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size is: torch.Size([4639, 10, 13]) \n",
      " test data size istorch.Size([515, 10, 13]) \n",
      " the data class is<class 'torch.Tensor'>\n",
      "Epoch [1/400], Loss: 54018.8086\n",
      "Epoch [2/400], Loss: 28569.7773\n",
      "Epoch [3/400], Loss: 21855.0352\n",
      "Epoch [4/400], Loss: 12933.3857\n",
      "Epoch [5/400], Loss: 11207.0537\n",
      "Epoch [6/400], Loss: 8013.4316\n",
      "Epoch [7/400], Loss: 8288.8330\n",
      "Epoch [8/400], Loss: 5737.7104\n",
      "Epoch [9/400], Loss: 4982.7686\n",
      "Epoch [10/400], Loss: 4672.0557\n",
      "Epoch [11/400], Loss: 4625.1328\n",
      "Epoch [12/400], Loss: 4857.8071\n",
      "Epoch [13/400], Loss: 3861.3149\n",
      "Epoch [14/400], Loss: 3585.7346\n",
      "Epoch [15/400], Loss: 3504.2905\n",
      "Epoch [16/400], Loss: 3035.5073\n",
      "Epoch [17/400], Loss: 1865.3651\n",
      "Epoch [18/400], Loss: 1407.7333\n",
      "Epoch [19/400], Loss: 1278.5016\n",
      "Epoch [20/400], Loss: 1064.8459\n",
      "Epoch [21/400], Loss: 1261.1995\n",
      "Epoch [22/400], Loss: 1384.2944\n",
      "Epoch [23/400], Loss: 1059.1830\n",
      "Epoch [24/400], Loss: 1916.3916\n",
      "Epoch [25/400], Loss: 1490.2794\n",
      "Epoch [26/400], Loss: 1085.4941\n",
      "Epoch [27/400], Loss: 871.4537\n",
      "Epoch [28/400], Loss: 1163.2201\n",
      "Epoch [29/400], Loss: 1133.8403\n",
      "Epoch [30/400], Loss: 910.5405\n",
      "Epoch [31/400], Loss: 1429.8229\n",
      "Epoch [32/400], Loss: 885.7125\n",
      "Epoch [33/400], Loss: 1122.0883\n",
      "Epoch [34/400], Loss: 1219.9764\n",
      "Epoch [35/400], Loss: 1358.7686\n",
      "Epoch [36/400], Loss: 816.7636\n",
      "Epoch [37/400], Loss: 718.8148\n",
      "Epoch [38/400], Loss: 1641.9589\n",
      "Epoch [39/400], Loss: 805.7494\n",
      "Epoch [40/400], Loss: 975.0591\n",
      "Epoch [41/400], Loss: 636.4755\n",
      "Epoch [42/400], Loss: 893.9350\n",
      "Epoch [43/400], Loss: 810.2831\n",
      "Epoch [44/400], Loss: 808.1172\n",
      "Epoch [45/400], Loss: 1704.0465\n",
      "Epoch [46/400], Loss: 1360.5568\n",
      "Epoch [47/400], Loss: 1077.1757\n",
      "Epoch [48/400], Loss: 947.8550\n",
      "Epoch [49/400], Loss: 921.3525\n",
      "Epoch [50/400], Loss: 1116.7332\n",
      "Epoch [51/400], Loss: 613.1628\n",
      "Epoch [52/400], Loss: 754.6866\n",
      "Epoch [53/400], Loss: 732.3583\n",
      "Epoch [54/400], Loss: 1343.5677\n",
      "Epoch [55/400], Loss: 661.6210\n",
      "Epoch [56/400], Loss: 595.3869\n",
      "Epoch [57/400], Loss: 835.1106\n",
      "Epoch [58/400], Loss: 683.4908\n",
      "Epoch [59/400], Loss: 712.4154\n",
      "Epoch [60/400], Loss: 846.1030\n",
      "Epoch [61/400], Loss: 876.4374\n",
      "Epoch [62/400], Loss: 589.3040\n",
      "Epoch [63/400], Loss: 754.4268\n",
      "Epoch [64/400], Loss: 566.4659\n",
      "Epoch [65/400], Loss: 674.5828\n",
      "Epoch [66/400], Loss: 766.3179\n",
      "Epoch [67/400], Loss: 10309.1924\n",
      "Epoch [68/400], Loss: 809.7317\n",
      "Epoch [69/400], Loss: 752.7092\n",
      "Epoch [70/400], Loss: 819.7865\n",
      "Epoch [71/400], Loss: 690.5202\n",
      "Epoch [72/400], Loss: 598.9196\n",
      "Epoch [73/400], Loss: 587.6148\n",
      "Epoch [74/400], Loss: 668.4070\n",
      "Epoch [75/400], Loss: 533.8619\n",
      "Epoch [76/400], Loss: 535.0363\n",
      "Epoch [77/400], Loss: 630.7864\n",
      "Epoch [78/400], Loss: 11477.2949\n",
      "Epoch [79/400], Loss: 732.5336\n",
      "Epoch [80/400], Loss: 747.6438\n",
      "Epoch [81/400], Loss: 540.8019\n",
      "Epoch [82/400], Loss: 581.5905\n",
      "Epoch [83/400], Loss: 640.3549\n",
      "Epoch [84/400], Loss: 501.6941\n",
      "Epoch [85/400], Loss: 529.8063\n",
      "Epoch [86/400], Loss: 701.8375\n",
      "Epoch [87/400], Loss: 461.5125\n",
      "Epoch [88/400], Loss: 620.2894\n",
      "Epoch [89/400], Loss: 504.7729\n",
      "Epoch [90/400], Loss: 375.5392\n",
      "Epoch [91/400], Loss: 745.5630\n",
      "Epoch [92/400], Loss: 468.8480\n",
      "Epoch [93/400], Loss: 468.8614\n",
      "Epoch [94/400], Loss: 2178.8833\n",
      "Epoch [95/400], Loss: 353.9835\n",
      "Epoch [96/400], Loss: 516.8414\n",
      "Epoch [97/400], Loss: 1929.2388\n",
      "Epoch [98/400], Loss: 742.3113\n",
      "Epoch [99/400], Loss: 407.7844\n",
      "Epoch [100/400], Loss: 457.0800\n",
      "Epoch [101/400], Loss: 395.3197\n",
      "Epoch [102/400], Loss: 365.1578\n",
      "Epoch [103/400], Loss: 469.4398\n",
      "Epoch [104/400], Loss: 503.6677\n",
      "Epoch [105/400], Loss: 413.6354\n",
      "Epoch [106/400], Loss: 491.3944\n",
      "Epoch [107/400], Loss: 482.7586\n",
      "Epoch [108/400], Loss: 400.7238\n",
      "Epoch [109/400], Loss: 327.0479\n",
      "Epoch [110/400], Loss: 794.4172\n",
      "Epoch [111/400], Loss: 507.3455\n",
      "Epoch [112/400], Loss: 549.6964\n",
      "Epoch [113/400], Loss: 589.6873\n",
      "Epoch [114/400], Loss: 6355.4736\n",
      "Epoch [115/400], Loss: 810.2245\n",
      "Epoch [116/400], Loss: 542.6337\n",
      "Epoch [117/400], Loss: 410.3239\n",
      "Epoch [118/400], Loss: 358.1698\n",
      "Epoch [119/400], Loss: 298.8001\n",
      "Epoch [120/400], Loss: 319.2921\n",
      "Epoch [121/400], Loss: 289.7830\n",
      "Epoch [122/400], Loss: 261.8679\n",
      "Epoch [123/400], Loss: 288.3835\n",
      "Epoch [124/400], Loss: 338.1570\n",
      "Epoch [125/400], Loss: 392.3372\n",
      "Epoch [126/400], Loss: 307.1583\n",
      "Epoch [127/400], Loss: 277.5976\n",
      "Epoch [128/400], Loss: 223.1064\n",
      "Epoch [129/400], Loss: 311.2858\n",
      "Epoch [130/400], Loss: 249.6576\n",
      "Epoch [131/400], Loss: 383.1407\n",
      "Epoch [132/400], Loss: 243.5971\n",
      "Epoch [133/400], Loss: 315.0656\n",
      "Epoch [134/400], Loss: 300.2154\n",
      "Epoch [135/400], Loss: 426.6417\n",
      "Epoch [136/400], Loss: 276.0185\n",
      "Epoch [137/400], Loss: 326.3757\n",
      "Epoch [138/400], Loss: 267.2853\n",
      "Epoch [139/400], Loss: 267.0240\n",
      "Epoch [140/400], Loss: 418.9935\n",
      "Epoch [141/400], Loss: 250.6039\n",
      "Epoch [142/400], Loss: 309.5035\n",
      "Epoch [143/400], Loss: 323.3966\n",
      "Epoch [144/400], Loss: 239.5771\n",
      "Epoch [145/400], Loss: 317.3370\n",
      "Epoch [146/400], Loss: 294.4273\n",
      "Epoch [147/400], Loss: 266.6157\n",
      "Epoch [148/400], Loss: 237.3934\n",
      "Epoch [149/400], Loss: 244.0003\n",
      "Epoch [150/400], Loss: 283.6058\n",
      "Epoch [151/400], Loss: 257.1312\n",
      "Epoch [152/400], Loss: 373.1938\n",
      "Epoch [153/400], Loss: 236.8363\n",
      "Epoch [154/400], Loss: 261.2844\n",
      "Epoch [155/400], Loss: 367.0350\n",
      "Epoch [156/400], Loss: 211.6703\n",
      "Epoch [157/400], Loss: 221.9896\n",
      "Epoch [158/400], Loss: 209.9459\n",
      "Epoch [159/400], Loss: 290.6002\n",
      "Epoch [160/400], Loss: 305.2632\n",
      "Epoch [161/400], Loss: 243.4299\n",
      "Epoch [162/400], Loss: 220.1280\n",
      "Epoch [163/400], Loss: 292.2552\n",
      "Epoch [164/400], Loss: 757.3116\n",
      "Epoch [165/400], Loss: 231.5247\n",
      "Epoch [166/400], Loss: 2937.8489\n",
      "Epoch [167/400], Loss: 452.0930\n",
      "Epoch [168/400], Loss: 304.9427\n",
      "Epoch [169/400], Loss: 296.2718\n",
      "Epoch [170/400], Loss: 257.1090\n",
      "Epoch [171/400], Loss: 326.0956\n",
      "Epoch [172/400], Loss: 218.1193\n",
      "Epoch [173/400], Loss: 349.2581\n",
      "Epoch [174/400], Loss: 221.9736\n",
      "Epoch [175/400], Loss: 243.6609\n",
      "Epoch [176/400], Loss: 290.0572\n",
      "Epoch [177/400], Loss: 239.9427\n",
      "Epoch [178/400], Loss: 260.2182\n",
      "Epoch [179/400], Loss: 370.7140\n",
      "Epoch [180/400], Loss: 307.7089\n",
      "Epoch [181/400], Loss: 259.9561\n",
      "Epoch [182/400], Loss: 296.1571\n",
      "Epoch [183/400], Loss: 257.2160\n",
      "Epoch [184/400], Loss: 298.4102\n",
      "Epoch [185/400], Loss: 285.0434\n",
      "Epoch [186/400], Loss: 203.5881\n",
      "Epoch [187/400], Loss: 244.7843\n",
      "Epoch [188/400], Loss: 246.2803\n",
      "Epoch [189/400], Loss: 344.3576\n",
      "Epoch [190/400], Loss: 295.8610\n",
      "Epoch [191/400], Loss: 215.3756\n",
      "Epoch [192/400], Loss: 168.8467\n",
      "Epoch [193/400], Loss: 254.1449\n",
      "Epoch [194/400], Loss: 323.2010\n",
      "Epoch [195/400], Loss: 268.7441\n",
      "Epoch [196/400], Loss: 208.7259\n",
      "Epoch [197/400], Loss: 342.3128\n",
      "Epoch [198/400], Loss: 255.0949\n",
      "Epoch [199/400], Loss: 287.1108\n",
      "Epoch [200/400], Loss: 178.1590\n",
      "Epoch [201/400], Loss: 207.8185\n",
      "Epoch [202/400], Loss: 245.2583\n",
      "Epoch [203/400], Loss: 205.2561\n",
      "Epoch [204/400], Loss: 237.7144\n",
      "Epoch [205/400], Loss: 187.0805\n",
      "Epoch [206/400], Loss: 237.2943\n",
      "Epoch [207/400], Loss: 233.1816\n",
      "Epoch [208/400], Loss: 211.6078\n",
      "Epoch [209/400], Loss: 242.7797\n",
      "Epoch [210/400], Loss: 182.6483\n",
      "Epoch [211/400], Loss: 628.6339\n",
      "Epoch [212/400], Loss: 198.1184\n",
      "Epoch [213/400], Loss: 205.1187\n",
      "Epoch [214/400], Loss: 208.0992\n",
      "Epoch [215/400], Loss: 333.8887\n",
      "Epoch [216/400], Loss: 221.9179\n",
      "Epoch [217/400], Loss: 246.2341\n",
      "Epoch [218/400], Loss: 226.0724\n",
      "Epoch [219/400], Loss: 239.3518\n",
      "Epoch [220/400], Loss: 265.1905\n",
      "Epoch [221/400], Loss: 191.4066\n",
      "Epoch [222/400], Loss: 211.9081\n",
      "Epoch [223/400], Loss: 207.4629\n",
      "Epoch [224/400], Loss: 253.7894\n",
      "Epoch [225/400], Loss: 201.2316\n",
      "Epoch [226/400], Loss: 217.8944\n",
      "Epoch [227/400], Loss: 208.8467\n",
      "Epoch [228/400], Loss: 179.4616\n",
      "Epoch [229/400], Loss: 170.6736\n",
      "Epoch [230/400], Loss: 213.7722\n",
      "Epoch [231/400], Loss: 209.1094\n",
      "Epoch [232/400], Loss: 255.5602\n",
      "Epoch [233/400], Loss: 245.0058\n",
      "Epoch [234/400], Loss: 203.6340\n",
      "Epoch [235/400], Loss: 154.2725\n",
      "Epoch [236/400], Loss: 223.3806\n",
      "Epoch [237/400], Loss: 209.9242\n",
      "Epoch [238/400], Loss: 233.7843\n",
      "Epoch [239/400], Loss: 312.7380\n",
      "Epoch [240/400], Loss: 204.1094\n",
      "Epoch [241/400], Loss: 198.2392\n",
      "Epoch [242/400], Loss: 338.2769\n",
      "Epoch [243/400], Loss: 222.4533\n",
      "Epoch [244/400], Loss: 202.6215\n",
      "Epoch [245/400], Loss: 195.9451\n",
      "Epoch [246/400], Loss: 283.7541\n",
      "Epoch [247/400], Loss: 241.4723\n",
      "Epoch [248/400], Loss: 143.5112\n",
      "Epoch [249/400], Loss: 252.1780\n",
      "Epoch [250/400], Loss: 222.1510\n",
      "Epoch [251/400], Loss: 239.8757\n",
      "Epoch [252/400], Loss: 194.5437\n",
      "Epoch [253/400], Loss: 192.3419\n",
      "Epoch [254/400], Loss: 189.6390\n",
      "Epoch [255/400], Loss: 219.6802\n",
      "Epoch [256/400], Loss: 149.1038\n",
      "Epoch [257/400], Loss: 313.5690\n",
      "Epoch [258/400], Loss: 187.0382\n",
      "Epoch [259/400], Loss: 164.7332\n",
      "Epoch [260/400], Loss: 233.7929\n",
      "Epoch [261/400], Loss: 342.3531\n",
      "Epoch [262/400], Loss: 265.9386\n",
      "Epoch [263/400], Loss: 209.6097\n",
      "Epoch [264/400], Loss: 440.8492\n",
      "Epoch [265/400], Loss: 209.6593\n",
      "Epoch [266/400], Loss: 226.8312\n",
      "Epoch [267/400], Loss: 228.8805\n",
      "Epoch [268/400], Loss: 153.0956\n",
      "Epoch [269/400], Loss: 287.0852\n",
      "Epoch [270/400], Loss: 251.0543\n",
      "Epoch [271/400], Loss: 199.6748\n",
      "Epoch [272/400], Loss: 204.2564\n",
      "Epoch [273/400], Loss: 213.4004\n",
      "Epoch [274/400], Loss: 208.0077\n",
      "Epoch [275/400], Loss: 194.1263\n",
      "Epoch [276/400], Loss: 256.7170\n",
      "Epoch [277/400], Loss: 211.0562\n",
      "Epoch [278/400], Loss: 228.6581\n",
      "Epoch [279/400], Loss: 230.4348\n",
      "Epoch [280/400], Loss: 202.8085\n",
      "Epoch [281/400], Loss: 218.1280\n",
      "Epoch [282/400], Loss: 202.9348\n",
      "Epoch [283/400], Loss: 228.4821\n",
      "Epoch [284/400], Loss: 227.6476\n",
      "Epoch [285/400], Loss: 231.6949\n",
      "Epoch [286/400], Loss: 213.9018\n",
      "Epoch [287/400], Loss: 191.4429\n",
      "Epoch [288/400], Loss: 177.2960\n",
      "Epoch [289/400], Loss: 207.9846\n",
      "Epoch [290/400], Loss: 183.7531\n",
      "Epoch [291/400], Loss: 213.9511\n",
      "Epoch [292/400], Loss: 224.5306\n",
      "Epoch [293/400], Loss: 199.3954\n",
      "Epoch [294/400], Loss: 230.3121\n",
      "Epoch [295/400], Loss: 156.4147\n",
      "Epoch [296/400], Loss: 199.7623\n",
      "Epoch [297/400], Loss: 160.5721\n",
      "Epoch [298/400], Loss: 295.3228\n",
      "Epoch [299/400], Loss: 232.0111\n",
      "Epoch [300/400], Loss: 205.0581\n",
      "Epoch [301/400], Loss: 164.3148\n",
      "Epoch [302/400], Loss: 176.7918\n",
      "Epoch [303/400], Loss: 194.3304\n",
      "Epoch [304/400], Loss: 176.7894\n",
      "Epoch [305/400], Loss: 255.3694\n",
      "Epoch [306/400], Loss: 204.4982\n",
      "Epoch [307/400], Loss: 213.0235\n",
      "Epoch [308/400], Loss: 244.3828\n",
      "Epoch [309/400], Loss: 229.8814\n",
      "Epoch [310/400], Loss: 233.4118\n",
      "Epoch [311/400], Loss: 222.5036\n",
      "Epoch [312/400], Loss: 229.9616\n",
      "Epoch [313/400], Loss: 183.5964\n",
      "Epoch [314/400], Loss: 177.5650\n",
      "Epoch [315/400], Loss: 194.2487\n",
      "Epoch [316/400], Loss: 191.4879\n",
      "Epoch [317/400], Loss: 137.8054\n",
      "Epoch [318/400], Loss: 171.0817\n",
      "Epoch [319/400], Loss: 259.1967\n",
      "Epoch [320/400], Loss: 221.2344\n",
      "Epoch [321/400], Loss: 192.9069\n",
      "Epoch [322/400], Loss: 185.7899\n",
      "Epoch [323/400], Loss: 722.3323\n",
      "Epoch [324/400], Loss: 188.1520\n",
      "Epoch [325/400], Loss: 208.8492\n",
      "Epoch [326/400], Loss: 171.8972\n",
      "Epoch [327/400], Loss: 249.0774\n",
      "Epoch [328/400], Loss: 265.7801\n",
      "Epoch [329/400], Loss: 203.3771\n",
      "Epoch [330/400], Loss: 211.7634\n",
      "Epoch [331/400], Loss: 163.9966\n",
      "Epoch [332/400], Loss: 196.0101\n",
      "Epoch [333/400], Loss: 175.4417\n",
      "Epoch [334/400], Loss: 211.5396\n",
      "Epoch [335/400], Loss: 172.3658\n",
      "Epoch [336/400], Loss: 193.9215\n",
      "Epoch [337/400], Loss: 185.2232\n",
      "Epoch [338/400], Loss: 204.8152\n",
      "Epoch [339/400], Loss: 182.1330\n",
      "Epoch [340/400], Loss: 198.1008\n",
      "Epoch [341/400], Loss: 186.7017\n",
      "Epoch [342/400], Loss: 214.2391\n",
      "Epoch [343/400], Loss: 187.1249\n",
      "Epoch [344/400], Loss: 191.1354\n",
      "Epoch [345/400], Loss: 396.2617\n",
      "Epoch [346/400], Loss: 233.1482\n",
      "Epoch [347/400], Loss: 232.2053\n",
      "Epoch [348/400], Loss: 169.8577\n",
      "Epoch [349/400], Loss: 164.9823\n",
      "Epoch [350/400], Loss: 213.2297\n",
      "Epoch [351/400], Loss: 135.1646\n",
      "Epoch [352/400], Loss: 252.0480\n",
      "Epoch [353/400], Loss: 139.1205\n",
      "Epoch [354/400], Loss: 154.3215\n",
      "Epoch [355/400], Loss: 179.7846\n",
      "Epoch [356/400], Loss: 193.4441\n",
      "Epoch [357/400], Loss: 198.8153\n",
      "Epoch [358/400], Loss: 157.3691\n",
      "Epoch [359/400], Loss: 166.1730\n",
      "Epoch [360/400], Loss: 249.7896\n",
      "Epoch [361/400], Loss: 270.7903\n",
      "Epoch [362/400], Loss: 178.9911\n",
      "Epoch [363/400], Loss: 155.9088\n",
      "Epoch [364/400], Loss: 228.0315\n",
      "Epoch [365/400], Loss: 150.9078\n",
      "Epoch [366/400], Loss: 172.5267\n",
      "Epoch [367/400], Loss: 144.1122\n",
      "Epoch [368/400], Loss: 179.6063\n",
      "Epoch [369/400], Loss: 184.8867\n",
      "Epoch [370/400], Loss: 134.8926\n",
      "Epoch [371/400], Loss: 179.4636\n",
      "Epoch [372/400], Loss: 241.1984\n",
      "Epoch [373/400], Loss: 190.3436\n",
      "Epoch [374/400], Loss: 199.2579\n",
      "Epoch [375/400], Loss: 155.0507\n",
      "Epoch [376/400], Loss: 162.7767\n",
      "Epoch [377/400], Loss: 228.8931\n",
      "Epoch [378/400], Loss: 164.0504\n",
      "Epoch [379/400], Loss: 194.3211\n",
      "Epoch [380/400], Loss: 159.0840\n",
      "Epoch [381/400], Loss: 243.1738\n",
      "Epoch [382/400], Loss: 153.2306\n",
      "Epoch [383/400], Loss: 164.4841\n",
      "Epoch [384/400], Loss: 165.4274\n",
      "Epoch [385/400], Loss: 239.5115\n",
      "Epoch [386/400], Loss: 221.3527\n",
      "Epoch [387/400], Loss: 183.1351\n",
      "Epoch [388/400], Loss: 163.2103\n",
      "Epoch [389/400], Loss: 132.7948\n",
      "Epoch [390/400], Loss: 172.7787\n",
      "Epoch [391/400], Loss: 157.7445\n",
      "Epoch [392/400], Loss: 285.8729\n",
      "Epoch [393/400], Loss: 183.1520\n",
      "Epoch [394/400], Loss: 179.1562\n",
      "Epoch [395/400], Loss: 231.3207\n",
      "Epoch [396/400], Loss: 233.6814\n",
      "Epoch [397/400], Loss: 253.9249\n",
      "Epoch [398/400], Loss: 139.8302\n",
      "Epoch [399/400], Loss: 139.9139\n",
      "Epoch [400/400], Loss: 153.4241\n"
     ]
    }
   ],
   "source": [
    "# run to train\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "file_path = './data/VBOX.csv'\n",
    "data = load_data(file_path)\n",
    "\n",
    "# spilt the data\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# set the T and process data\n",
    "T = 10  # 1s\n",
    "train_data, test_data = switch_data(train_data, T), switch_data(test_data, T)\n",
    "train_data, test_data = torch.tensor(train_data, dtype=torch.float32).to(device), torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "input_dimension = train_data.shape[-1]\n",
    "hidden_dimension = input_dimension // 2 - 1\n",
    "print(f\"train data size is: {train_data.shape}\", \"\\n\", f\"test data size is{test_data.shape}\", \"\\n\", f\"the data class is{type(train_data)}\")\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# encoder = Encoder(input_dimension, hidden_dimension).to(device)                  \n",
    "# encoder_output = encoder(train_data[0:32, :, :])\n",
    "# print(encoder_output)\n",
    "\n",
    "# decoder = Decoder(hidden_dimension, input_dimension).to(device)\n",
    "# decoder_output = decoder(encoder_output, encoder_output, encoder_output)\n",
    "# print(f\"decoder_output size is:{decoder_output.shape}\")\n",
    "# print(decoder_output)\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(train_data)\n",
    "test_dataset = TensorDataset(test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 创建模型\n",
    "num_layers = 2\n",
    "autoencoder = AutoEncoder(input_dimension, hidden_dimension, num_layers).to(device)\n",
    "\n",
    "# 训练模型\n",
    "train_model(autoencoder, train_loader, device, num_epochs=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
